{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description\n",
    "\n",
    "Run the XGBoost algorithm for LDEO-HPD Reconstruction \n",
    "\n",
    "Uses best parameters from Luke's work\n",
    "\n",
    "GCB Models 2020 version - through 2019\n",
    "\n",
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# For accessing directories\n",
    "# =========================================\n",
    "root_dir = \"/data/artemis/workspace/vbennington/LDEO_HPD/models/XGB/GCB_2021\"  # directory output will be written to  \n",
    "\n",
    "data_output_dir = f\"/data/artemis/workspace/vbennington/LDEO_HPD/2021models/data\"  # directory with features and pCO2\n",
    "model_output_dir = f\"{root_dir}/trained\"  # trained models will be saved here\n",
    "recon_output_dir = f\"{root_dir}/reconstructions\" # reconstructions will be saved here\n",
    "other_output_dir = f\"{root_dir}/performance_metrics\" # performance metrics will be saved here\n",
    "\n",
    "approach = 'xg'\n",
    "# =========================================\n",
    "# Number of cores you have access to for model training\n",
    "# =========================================\n",
    "jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'jpg'\n",
    "%config InlineBackend.print_figure_kwargs = {'dpi':300, 'bbox_inches': 'tight'}\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn.linear_model \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "import xgboost as xgb     # extreme gradient boosting (XGB\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import sklearn.model_selection as mselect\n",
    "\n",
    "# Python file with supporting functions\n",
    "import pre_HPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predefined Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Defining some inputs for the modeling process\n",
    "# =========================================\n",
    "\n",
    "# Train-validate-test split proportions for LET runs:\n",
    "val_prop = .2\n",
    "test_prop = .2\n",
    "\n",
    "# Parameters from Luke's work for consistency:\n",
    "param_best = {        'random_state':42,\n",
    "                      'max_depth': 9, \n",
    "                      'min_child_weight': 1, \n",
    "                      'gamma': 0,\n",
    "                      'subsample': 0.85,\n",
    "                      'colsample_bytree': 0.95,\n",
    "                      'reg_alpha': 0.09,\n",
    "                      'reg_lambda': 1,\n",
    "                      'n_estimators': 1500,\n",
    "                      'learning_rate': 0.05,\n",
    "                     }\n",
    "\n",
    "# Feature and target lists for feeding into ML\n",
    "features_sel = ['sst','sst_anom','SSS','sss_anom','chl_log','chl_anom','mld_log','XCO2','A','B','C','T0','T1']\n",
    "target_sel = ['error']  # What we reconstruct with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ 'cesm_sfco2_1x1_A', \n",
    "               #'csiro_spco2_1x1_A',\n",
    "               'fesom2_sfco2_1x1_A',\n",
    "               'mpi_sfco2_1x1_A', \n",
    "               'cnrm_sfco2_1x1_A',\n",
    "               'ipsl_sfco2_1x1_A',\n",
    "               'planktom_sfco2_1x1_A',\n",
    "               'noresm_sfco2_1x1_A',\n",
    "               'princeton_sfco2_1x1_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates of Reconstruction \n",
    "date_range_start = '1982-01-01T00:00:00.000000000'\n",
    "date_range_end = '2020-12-01T00:00:00.000000000'\n",
    "\n",
    "# create date vector\n",
    "dates = pd.date_range(start=date_range_start, \n",
    "                      end=date_range_end,freq='MS') + np.timedelta64(14, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [97, 62, 12] # LDEO HPD\n",
    "random_seeds = [22, 15, 84] # 2021 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-02 16:32:26.212680\n",
      "Done opening data file\n",
      "Dropped unused columns\n",
      "cesm_sfco2_1x1_A\n",
      "Obs. length = 278448\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 162.85638017640937, 'mae': 8.329405159635293, 'medae': 5.521587149289104, 'max_error': 202.13710386769168, 'bias': -0.10451801717700437, 'r2': 0.8344498646908731, 'corr': 0.9141307235905182, 'cent_rmse': 12.761091500740879, 'stdev': 27.596539, 'amp_ratio': 0.6835614382290343, 'stdev_ref': 31.364445844246983, 'range_ref': 484.7129558148982, 'iqr_ref': 31.21303327745595}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "fesom2_sfco2_1x1_A\n",
      "Obs. length = 280844\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 179.58448322811398, 'mae': 8.827463033152462, 'medae': 6.036677794576008, 'max_error': 189.7687765860457, 'bias': 0.06690025441022573, 'r2': 0.8657027504240838, 'corr': 0.9306248462569876, 'cent_rmse': 13.400746512417669, 'stdev': 33.340607, 'amp_ratio': 0.8407536255814106, 'stdev_ref': 36.567969450416086, 'range_ref': 481.6848713817873, 'iqr_ref': 42.46152912829825}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "mpi_sfco2_1x1_A\n",
      "Obs. length = 276136\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 211.32335646584818, 'mae': 9.444988224387778, 'medae': 6.2956802767914155, 'max_error': 237.60533572353359, 'bias': 0.03358830812962843, 'r2': 0.8302437585317425, 'corr': 0.9122330315392022, 'cent_rmse': 14.5369264078489, 'stdev': 30.638182, 'amp_ratio': 0.7989341350184092, 'stdev_ref': 35.28262383331044, 'range_ref': 505.416042097969, 'iqr_ref': 34.25045012278976}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "cnrm_sfco2_1x1_A\n",
      "Obs. length = 280843\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 176.48788008791897, 'mae': 8.7451328478704, 'medae': 5.890210148997312, 'max_error': 228.1771822885555, 'bias': -0.004186494627249571, 'r2': 0.8173100957412445, 'corr': 0.9050161244128191, 'cent_rmse': 13.284873444260572, 'stdev': 26.83112, 'amp_ratio': 0.7652724516813024, 'stdev_ref': 31.08137113465882, 'range_ref': 455.65409176782583, 'iqr_ref': 32.564333757647034}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "ipsl_sfco2_1x1_A\n",
      "Obs. length = 280843\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 167.88656401607298, 'mae': 8.40177762739636, 'medae': 5.595866844715033, 'max_error': 220.14895939836117, 'bias': 0.05252521740635041, 'r2': 0.8068992119035605, 'corr': 0.899236755279694, 'cent_rmse': 12.956998313901448, 'stdev': 25.290482, 'amp_ratio': 0.7400384549576864, 'stdev_ref': 29.48600584659681, 'range_ref': 479.1356368190255, 'iqr_ref': 29.54731017924223}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "planktom_sfco2_1x1_A\n",
      "Obs. length = 278038\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 187.73416094681946, 'mae': 9.11563596868233, 'medae': 6.221277360145223, 'max_error': 237.19840159914918, 'bias': -0.054819983089978264, 'r2': 0.8438631586253488, 'corr': 0.9195568770455207, 'cent_rmse': 13.7015019502698, 'stdev': 30.448074, 'amp_ratio': 0.7257181536222626, 'stdev_ref': 34.675198691008376, 'range_ref': 466.69835334096615, 'iqr_ref': 36.92893360620849}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "noresm_sfco2_1x1_A\n",
      "Obs. length = 280844\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 174.76324508477236, 'mae': 8.598532828767214, 'medae': 5.810119647228532, 'max_error': 217.73799007176348, 'bias': 0.03882530063093892, 'r2': 0.7935910997011164, 'corr': 0.8919409215810961, 'cent_rmse': 13.219748009069708, 'stdev': 24.663464, 'amp_ratio': 0.797365713349415, 'stdev_ref': 29.097845938618814, 'range_ref': 422.21686893035945, 'iqr_ref': 31.573854049185627}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "princeton_sfco2_1x1_A\n",
      "Obs. length = 278249\n",
      "Starting model saving process\n",
      "Save complete\n",
      "{'mse': 182.90793761208272, 'mae': 8.864945732170817, 'medae': 5.997438559417958, 'max_error': 226.80340963119556, 'bias': -0.05589872208037949, 'r2': 0.7619273429360538, 'corr': 0.8749096552807987, 'cent_rmse': 13.524230596817098, 'stdev': 22.602613, 'amp_ratio': 0.6598334519528004, 'stdev_ref': 27.717976161488075, 'range_ref': 558.9901390643504, 'iqr_ref': 29.706473688417447}\n",
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "2022-03-02 17:22:43.924730\n"
     ]
    }
   ],
   "source": [
    "best_params = param_best # Luke's parameters\n",
    "test_performance = defaultdict(dict)\n",
    "K_folds = 3\n",
    "target_sel= ['error']\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Data file path\n",
    "data_dir = f\"{data_output_dir}\"\n",
    "fname = f\"data_clean_2D_mon_1x1_198201-202012.pkl\"\n",
    "file_path = f\"{data_dir}/{fname}\"\n",
    "        \n",
    "# Read in data, create some selection filters, produce a reduced dataframe\n",
    "# Same for all models!\n",
    "df = pd.read_pickle(file_path)\n",
    "print(\"Done opening data file\")\n",
    "\n",
    "# Get rid of features we never use:\n",
    "df = df.drop(columns=['chl','mld'])\n",
    "print(\"Dropped unused columns\")\n",
    "\n",
    "# Now, loop on the models to reconstruct the model-obs difference:\n",
    "\n",
    "for mod in models:\n",
    "    print(mod)\n",
    "    \n",
    "    df['error'] =  df['pCO2'] - df[f\"{mod}\"]\n",
    "    \n",
    "    # Get rid of NaN    \n",
    "    # Same for all models\n",
    "    recon_sel = (~df[features_sel+['net_mask']].isna().any(axis=1))   # Dont' have pCO2 for non-SOCAT locations, but have features\n",
    "\n",
    "    ##################################################################################################################################        \n",
    "    sel = (recon_sel & (df['socat_mask'] == 1) & (~np.isnan(df['error']))).to_numpy().ravel()           # locations not masked AND IN SOCAT SAMPLING\n",
    "    print(\"Obs. length =\",sum(sel))\n",
    "    ###################################################################################################################################\n",
    " \n",
    "    # Convert dataframe to numpy arrays, train/val/test split\n",
    "    X = df.loc[sel,features_sel].to_numpy()\n",
    "    y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "    \n",
    "    # Where we want pCO2 reconstructed\n",
    "    X_recon = df.loc[recon_sel,features_sel].to_numpy()   \n",
    "    \n",
    "    ###################################################################################################################################\n",
    "    # Separate the data sets\n",
    "    ###################################################################################################################################\n",
    "    ###################################################################################################################################    \n",
    "    # Uses train_test_split build into sklearn.model_selection\n",
    "    N = X.shape[0]\n",
    "    train_val_idx, train_idx, val_idx, test_idx = pre_HPD.train_val_test_split(N, test_prop, val_prop, random_seeds)\n",
    "    X_train_val, X_train, X_val, X_test, y_train_val, y_train, y_val, y_test = pre_HPD.apply_splits(X, y, train_val_idx, train_idx, val_idx, test_idx) \n",
    "      \n",
    "    # Fit the model on train/validation data\n",
    "    model = XGBRegressor(**best_params, n_jobs=jobs)\n",
    "    model.fit(X_train_val, y_train_val)          \n",
    "\n",
    "    # Save the model\n",
    "    pre_HPD.save_model(model, mod, model_output_dir, approach)   #Uncomment when actually running\n",
    "           \n",
    "    # Calculate some test error metrics and store in a dictionary\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    # for pCO2-model difference\n",
    "    test_performance[mod] = pre_HPD.evaluate_test(y_test, y_pred_test)\n",
    "    print(test_performance[mod])\n",
    "\n",
    "    ######################################################################################\n",
    "    # Reconstruct Everywhere with Trained Model\n",
    "    ######################################################################################\n",
    "    # Everywhere, SOCAT and non-SOCAT ####################################################\n",
    "    y_recon = model.predict(X_recon)\n",
    "\n",
    "    # Full reconstruction ##\n",
    "    df[f'error_{mod}'] = np.nan\n",
    "    df.loc[recon_sel,[f'error_{mod}']] = y_recon   \n",
    "        \n",
    "    DS_recon = df[['net_mask','socat_mask','pCO2', f'{mod}',f'error_{mod}']].to_xarray()\n",
    "\n",
    "    ########## SAVE ####################################################################################################\n",
    "    pre_HPD.save_recon(DS_recon, mod, recon_output_dir, approach)   # Uncomment when actually running\n",
    "\n",
    "    del y_test, y_recon, y, DS_recon\n",
    "    \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters and performance metrics\n",
    "approach_output_dir = f\"{other_output_dir}\"\n",
    "param_fname = f\"{approach_output_dir}/best_params_dict.pickle\"\n",
    "test_perform_fname = f\"{approach_output_dir}/test_performance_dict.pickle\"\n",
    "\n",
    "Path(approach_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(param_fname, 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(test_perform_fname, 'wb') as handle:\n",
    "    pickle.dump(test_performance, handle)\n",
    "    \n",
    "# Convert performance metrics to dataframes\n",
    "test_df = pd.DataFrame.from_dict(test_performance,\n",
    "                                 orient='index')\n",
    "\n",
    "# Save the dataframes too\n",
    "test_df_fname = f\"{approach_output_dir}/test_performance_df.pickle\"\n",
    "\n",
    "test_df.to_pickle(test_df_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EOF Test2",
   "language": "python",
   "name": "dev2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
